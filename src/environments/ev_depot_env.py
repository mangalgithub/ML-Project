"""
EV Depot Environment for Reinforcement Learning
Gymnasium-compatible environment for EV Fleet V2G Peak Shaving

State: Vehicle SoCs, time to departure, electricity prices, charger availability
Action: Continuous charge/discharge rates for each charger [-1, 1]
Reward: -energy_cost - peak_penalty - late_departure_penalty - battery_degradation

Author: EV Fleet RL Project
Date: October 2025
"""

import gymnasium as gym
from gymnasium import spaces
import numpy as np
import pandas as pd
from typing import Dict, Tuple, Optional, Any


class EVDepotEnv(gym.Env):
    """
    Gymnasium Environment for EV Depot Charging + V2G Peak Shaving
    
    The environment simulates a depot with multiple EVs and chargers.
    The goal is to minimize costs (energy + peak demand) while ensuring
    vehicles are ready for departure with sufficient charge.
    
    State Space:
        - Per vehicle: [SoC, time_to_departure_normalized, at_depot_flag]
        - Global: [current_price_normalized, grid_load_normalized, peak_so_far_normalized]
    
    Action Space:
        - Continuous actions for each charger: [-1, 1]
        - -1 = maximum V2G discharge
        -  0 = idle
        - +1 = maximum charge
    
    Reward:
        reward = -(energy_cost + peak_penalty + late_penalty + battery_cost)
    
    Example:
        >>> from data_generator import EVFleetDataGenerator
        >>> generator = EVFleetDataGenerator()
        >>> dataset = generator.generate_complete_dataset()
        >>> env = EVDepotEnv(dataset)
        >>> obs, info = env.reset()
        >>> action = env.action_space.sample()
        >>> obs, reward, terminated, truncated, info = env.step(action)
    """
    
    metadata = {"render_modes": ["human", "rgb_array"]}
    
    def __init__(self, 
                 dataset: Dict,
                 peak_penalty_coef: float = 100.0,
                 late_penalty_coef: float = 500.0,
                 battery_cycle_cost: float = 0.05,
                 render_mode: Optional[str] = None):
        """
        Initialize the EV Depot Environment
        
        Args:
            dataset: Dictionary containing vehicles, schedules, prices, etc.
                    Generated by EVFleetDataGenerator
            peak_penalty_coef: Penalty coefficient for peak demand (€/kW)
            late_penalty_coef: Penalty for each late departure (€)
            battery_cycle_cost: Cost per kWh of battery cycling (€/kWh)
            render_mode: Rendering mode ('human', 'rgb_array', or None)
        """
        super().__init__()
        
        self.dataset = dataset
        self.vehicles = dataset['vehicles']
        self.schedules = dataset['schedules']
        self.prices = dataset['prices']
        self.base_load = dataset['base_load']
        self.chargers = dataset['chargers']
        
        self.n_vehicles = len(self.vehicles)
        self.n_chargers = len(self.chargers)
        self.timestep_hours = dataset['metadata']['timestep_minutes'] / 60.0
        self.total_steps = dataset['metadata']['total_steps']
        
        # Penalty coefficients
        self.peak_penalty_coef = peak_penalty_coef
        self.late_penalty_coef = late_penalty_coef
        self.battery_cycle_cost = battery_cycle_cost
        
        # Rendering
        self.render_mode = render_mode
        
        # State tracking
        self.current_step = 0
        self.vehicle_soc = np.zeros(self.n_vehicles)  # Current SoC (0-1)
        self.vehicle_at_depot = np.zeros(self.n_vehicles, dtype=bool)  # At depot?
        self.charger_assignment = -np.ones(self.n_vehicles, dtype=int)  # Which charger (-1 = none)
        
        # Performance tracking
        self.cumulative_cost = 0.0
        self.peak_demand = 0.0
        self.total_energy_charged = 0.0
        self.total_energy_discharged = 0.0
        self.late_departures = 0
        self.total_departures = 0
        self.episode_rewards = []
        
        # Episode history (for rendering/analysis)
        self.history = {
            'soc': [],
            'actions': [],
            'rewards': [],
            'costs': [],
            'grid_load': []
        }
        
        # Define observation space
        # For each vehicle: [soc, time_to_departure_normalized, at_depot]
        # Global: [current_price, grid_load_normalized, peak_so_far_normalized]
        obs_dim = self.n_vehicles * 3 + 3
        self.observation_space = spaces.Box(
            low=0.0, high=1.0, shape=(obs_dim,), dtype=np.float32
        )
        
        # Define action space
        # Continuous action for each charger: [-1, 1]
        # -1 = max discharge (V2G), 0 = idle, +1 = max charge
        self.action_space = spaces.Box(
            low=-1.0, high=1.0, shape=(self.n_chargers,), dtype=np.float32
        )
        
        print(f"✅ EVDepotEnv initialized")
        print(f"   Observation space: {self.observation_space.shape}")
        print(f"   Action space: {self.action_space.shape}")
        print(f"   Episode length: {self.total_steps} timesteps")
    
    def reset(self, seed: Optional[int] = None, options: Optional[dict] = None) -> Tuple[np.ndarray, Dict]:
        """
        Reset environment to initial state
        
        Args:
            seed: Random seed for reproducibility
            options: Additional options
            
        Returns:
            observation: Initial observation
            info: Additional information
        """
        super().reset(seed=seed)
        
        self.current_step = 0
        self.cumulative_cost = 0.0
        self.peak_demand = 0.0
        self.total_energy_charged = 0.0
        self.total_energy_discharged = 0.0
        self.late_departures = 0
        self.total_departures = 0
        
        # Initialize vehicles at depot with initial SoC
        self.vehicle_at_depot = np.ones(self.n_vehicles, dtype=bool)
        
        # Set initial SoC from first schedule entry for each vehicle
        for vid in range(self.n_vehicles):
            vehicle_schedules = self.schedules[self.schedules['vehicle_id'] == vid]
            if len(vehicle_schedules) > 0:
                first_schedule = vehicle_schedules.iloc[0]
                self.vehicle_soc[vid] = first_schedule['soc_at_arrival']
            else:
                self.vehicle_soc[vid] = 0.5  # Default 50% if no schedule
        
        self.charger_assignment = -np.ones(self.n_vehicles, dtype=int)
        
        # Reset history
        self.history = {
            'soc': [],
            'actions': [],
            'rewards': [],
            'costs': [],
            'grid_load': []
        }
        
        observation = self._get_observation()
        info = self._get_info()
        
        return observation, info
    
    def step(self, action: np.ndarray) -> Tuple[np.ndarray, float, bool, bool, Dict]:
        """
        Execute one timestep of the environment
        
        Args:
            action: Action to take (charge/discharge rates for each charger)
            
        Returns:
            observation: New observation after taking action
            reward: Reward for this step
            terminated: Whether episode is finished
            truncated: Whether episode was truncated
            info: Additional information
        """
        # Clip actions to valid range
        action = np.clip(action, -1.0, 1.0)
        
        # 1. Update vehicle arrivals/departures based on schedule
        self._update_vehicle_status()
        
        # 2. Assign vehicles to available chargers
        self._assign_chargers()
        
        # 3. Execute charging/discharging actions and calculate costs
        energy_cost, peak_penalty, late_penalty, battery_cost = self._execute_actions(action)
        
        # 4. Calculate reward (negative of total cost)
        total_cost = energy_cost + peak_penalty + late_penalty + battery_cost
        reward = -total_cost
        
        # 5. Update state
        self.current_step += 1
        self.cumulative_cost += total_cost
        
        # 6. Store history
        self.history['soc'].append(self.vehicle_soc.copy())
        self.history['actions'].append(action.copy())
        self.history['rewards'].append(reward)
        self.history['costs'].append(total_cost)
        
        # 7. Check if episode done
        terminated = self.current_step >= self.total_steps
        truncated = False
        
        observation = self._get_observation()
        info = self._get_info()
        info.update({
            'step': self.current_step,
            'energy_cost': energy_cost,
            'peak_penalty': peak_penalty,
            'late_penalty': late_penalty,
            'battery_cost': battery_cost,
            'total_cost': total_cost
        })
        
        if terminated:
            self.episode_rewards.append(self.cumulative_cost)
        
        return observation, reward, terminated, truncated, info
    
    def _get_observation(self) -> np.ndarray:
        """
        Construct observation vector from current state
        
        Returns:
            Observation vector (normalized to [0, 1])
        """
        obs = []
        
        # Per-vehicle information
        for vid in range(self.n_vehicles):
            # 1. State of Charge (0-1)
            obs.append(self.vehicle_soc[vid])
            
            # 2. Time to next departure (normalized by 1 day = 96 steps)
            if self.vehicle_at_depot[vid]:
                next_departure = self._get_next_departure(vid)
                if next_departure is not None:
                    time_to_dep = (next_departure - self.current_step) / 96.0
                    obs.append(np.clip(time_to_dep, 0, 1))
                else:
                    obs.append(1.0)  # Far future/no departure
            else:
                obs.append(0.0)  # Not at depot
            
            # 3. At depot flag (0 or 1)
            obs.append(float(self.vehicle_at_depot[vid]))
        
        # Global information
        # 1. Current electricity price (normalized by max expected price)
        # Ensure we don't exceed bounds
        step_idx = min(self.current_step, len(self.prices) - 1)
        current_price = self.prices.iloc[step_idx]['price_per_kwh']
        obs.append(np.clip(current_price / 0.5, 0, 1))
        
        # 2. Current base grid load (normalized)
        current_load = self.base_load.iloc[step_idx]['base_load_kw']
        obs.append(np.clip(current_load / 200.0, 0, 1))

        
        # 3. Peak demand so far (normalized)
        obs.append(np.clip(self.peak_demand / 500.0, 0, 1))
        
        return np.array(obs, dtype=np.float32)
    
    def _get_info(self) -> Dict[str, Any]:
        """
        Return additional information about current state
        
        Returns:
            Dictionary with metrics and state information
        """
        on_time_rate = 1.0 - (self.late_departures / max(1, self.total_departures))
        
        return {
            'timestep': self.current_step,
            'cumulative_cost': self.cumulative_cost,
            'peak_demand': self.peak_demand,
            'total_energy_charged': self.total_energy_charged,
            'total_energy_discharged': self.total_energy_discharged,
            'late_departures': self.late_departures,
            'total_departures': self.total_departures,
            'on_time_rate': on_time_rate,
            'vehicles_at_depot': self.vehicle_at_depot.sum(),
            'chargers_in_use': (self.charger_assignment >= 0).sum()
        }
    
    def _update_vehicle_status(self):
        """Update which vehicles are at depot based on schedule"""
        
        # Check for arrivals at current timestep
        arrivals = self.schedules[self.schedules['arrival_step'] == self.current_step]
        for _, row in arrivals.iterrows():
            vid = int(row['vehicle_id'])
            self.vehicle_at_depot[vid] = True
            self.vehicle_soc[vid] = row['soc_at_arrival']
        
        # Check for departures at current timestep
        departures = self.schedules[self.schedules['departure_step'] == self.current_step]
        for _, row in departures.iterrows():
            vid = int(row['vehicle_id'])
            
            # Check if SoC target was met
            target_soc = self.vehicles.iloc[vid]['soc_target']
            if self.vehicle_soc[vid] < target_soc:
                self.late_departures += 1
            
            self.total_departures += 1
            self.vehicle_at_depot[vid] = False
            
            # Remove charger assignment
            if self.charger_assignment[vid] >= 0:
                self.charger_assignment[vid] = -1
    
    def _assign_chargers(self):
        """
        Assign vehicles to available chargers using FCFS (First-Come-First-Serve)
        """
        
        # Find vehicles at depot without charger assignment
        vehicles_needing_charger = np.where(
            self.vehicle_at_depot & (self.charger_assignment == -1)
        )[0]
        
        # Find available chargers (not currently assigned)
        occupied_chargers = set(self.charger_assignment[self.charger_assignment >= 0])
        available_chargers = [c for c in range(self.n_chargers) if c not in occupied_chargers]
        
        # Assign chargers to vehicles (FCFS)
        for vid in vehicles_needing_charger:
            if len(available_chargers) > 0:
                charger = available_chargers.pop(0)
                self.charger_assignment[vid] = charger
    
    def _execute_actions(self, action: np.ndarray) -> Tuple[float, float, float, float]:
        """
        Execute charge/discharge actions and calculate costs
        
        Args:
            action: Charge/discharge rates for each charger
            
        Returns:
            energy_cost: Cost of electricity consumed/revenue from V2G (€)
            peak_penalty: Penalty for increasing peak demand (€)
            late_penalty: Penalty for late departures (€)
            battery_cost: Battery degradation cost (€)
        """
        
        energy_cost = 0.0
        peak_penalty = 0.0
        late_penalty = 0.0
        battery_cost = 0.0
        
        # Get current price and base load (with bounds checking)
        step_idx = min(self.current_step, len(self.prices) - 1)
        current_price = self.prices.iloc[step_idx]['price_per_kwh']
        v2g_price = self.prices.iloc[step_idx]['v2g_sell_price']
        current_base_load = self.base_load.iloc[step_idx]['base_load_kw']

        
        total_ev_power = 0.0  # Net EV power (positive = charging, negative = discharging)
        
        # Execute actions for each charger
        for cid in range(self.n_chargers):
            # Find vehicle assigned to this charger
            vid_array = np.where(self.charger_assignment == cid)[0]
            
            if len(vid_array) == 0:
                continue  # Charger not in use
            
            vid = vid_array[0]
            
            # Get vehicle specifications
            v_spec = self.vehicles.iloc[vid]
            capacity = v_spec['battery_capacity']
            max_charge_power = v_spec['max_charging_power']
            max_discharge_power = v_spec['max_discharging_power']
            charge_eff = v_spec['charging_efficiency']
            discharge_eff = v_spec['discharging_efficiency']
            soc_min = v_spec['soc_min']
            soc_max = v_spec['soc_max']
            
            # Interpret action value
            action_val = np.clip(action[cid], -1, 1)
            
            if action_val > 0:  # Charging
                # Calculate power and energy
                power = action_val * max_charge_power  # kW
                energy = power * self.timestep_hours  # kWh
                
                # Check SoC limits
                new_soc = self.vehicle_soc[vid] + (energy * charge_eff) / capacity
                if new_soc > soc_max:
                    new_soc = soc_max
                    energy = (new_soc - self.vehicle_soc[vid]) * capacity / charge_eff
                    power = energy / self.timestep_hours
                
                # Update state
                self.vehicle_soc[vid] = new_soc
                total_ev_power += power
                
                # Calculate costs
                energy_cost += energy * current_price
                battery_cost += energy * self.battery_cycle_cost
                self.total_energy_charged += energy
                
            elif action_val < 0:  # Discharging (V2G)
                # Calculate power and energy
                power = -action_val * max_discharge_power  # kW (positive value)
                energy = power * self.timestep_hours  # kWh
                
                # Check SoC limits
                new_soc = self.vehicle_soc[vid] - energy / (capacity * discharge_eff)
                if new_soc < soc_min:
                    new_soc = soc_min
                    energy = (self.vehicle_soc[vid] - new_soc) * capacity * discharge_eff
                    power = energy / self.timestep_hours
                
                # Update state
                self.vehicle_soc[vid] = new_soc
                total_ev_power -= power  # Negative for discharge
                
                # Calculate costs/revenue
                energy_cost -= energy * v2g_price  # Revenue from V2G (negative cost)
                battery_cost += energy * self.battery_cycle_cost * 1.5  # Higher degradation for discharge
                self.total_energy_discharged += energy
        
        # Calculate total grid power
        total_grid_power = current_base_load + total_ev_power
        
        # Store for history
        self.history['grid_load'].append(total_grid_power)
        
        # Update peak demand and calculate penalty
        if total_grid_power > self.peak_demand:
            peak_penalty = self.peak_penalty_coef * (total_grid_power - self.peak_demand)
            self.peak_demand = total_grid_power
        
        return energy_cost, peak_penalty, late_penalty, battery_cost
    
    def _get_next_departure(self, vid: int) -> Optional[int]:
        """
        Get next departure timestep for a vehicle
        
        Args:
            vid: Vehicle ID
            
        Returns:
            Departure timestep or None if no future departure
        """
        future_departures = self.schedules[
            (self.schedules['vehicle_id'] == vid) &
            (self.schedules['departure_step'] > self.current_step)
        ]
        
        if len(future_departures) > 0:
            return int(future_departures.iloc[0]['departure_step'])
        return None
    
    def render(self):
        """
        Render the environment
        
        For 'human' mode: Print status to console
        For 'rgb_array' mode: Return RGB array (not implemented)
        """
        if self.render_mode == "human":
            if self.current_step % 96 == 0:  # Print once per day
                day = self.current_step // 96 + 1
                print(f"\n{'='*70}")
                print(f"Day {day} - Timestep {self.current_step}/{self.total_steps}")
                print(f"{'='*70}")
                print(f"Vehicles at depot:  {self.vehicle_at_depot.sum()}/{self.n_vehicles}")
                print(f"Chargers in use:    {(self.charger_assignment >= 0).sum()}/{self.n_chargers}")
                print(f"Peak demand:        {self.peak_demand:.1f} kW")
                print(f"Cumulative cost:    €{self.cumulative_cost:.2f}")
                print(f"On-time rate:       {1.0 - self.late_departures/max(1, self.total_departures):.1%}")
                print(f"Avg vehicle SoC:    {self.vehicle_soc[self.vehicle_at_depot].mean():.1%}")
                print(f"Energy charged:     {self.total_energy_charged:.1f} kWh")
                print(f"Energy discharged:  {self.total_energy_discharged:.1f} kWh")
                print(f"{'='*70}")
        
        elif self.render_mode == "rgb_array":
            # Not implemented - would return visual representation
            pass
    
    def close(self):
        """Clean up environment resources"""
        pass
    
    def get_episode_metrics(self) -> Dict[str, float]:
        """
        Get comprehensive metrics for the completed episode
        
        Returns:
            Dictionary with performance metrics
        """
        avg_soc = np.mean([soc.mean() for soc in self.history['soc']])
        peak_load = max(self.history['grid_load']) if self.history['grid_load'] else 0
        
        return {
            'total_cost': self.cumulative_cost,
            'peak_demand': self.peak_demand,
            'on_time_rate': 1.0 - (self.late_departures / max(1, self.total_departures)),
            'late_departures': self.late_departures,
            'total_departures': self.total_departures,
            'energy_charged': self.total_energy_charged,
            'energy_discharged': self.total_energy_discharged,
            'net_energy': self.total_energy_charged - self.total_energy_discharged,
            'battery_cycles': self.total_energy_charged / (self.vehicles['battery_capacity'].mean() * 2),
            'average_soc': avg_soc,
            'peak_grid_load': peak_load,
            'episode_length': self.current_step
        }


# ============================================================================
# EXAMPLE USAGE AND TESTING
# ============================================================================

if __name__ == "__main__":
    print("""
    ╔═══════════════════════════════════════════════════════════════════╗
    ║           EV Depot Environment - Testing Script                  ║
    ╚═══════════════════════════════════════════════════════════════════╝
    """)
    
    # Load dataset
    import sys
    import os
# Add project root to path
    sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '../..')))
    from data_generator import EVFleetDataGenerator

    
    print("\n1. Loading dataset...")
    generator = EVFleetDataGenerator(
        n_vehicles=5,
        n_chargers=4,
        simulation_days=2,
        seed=42
    )
    dataset = generator.generate_complete_dataset()
    
    # Create environment
    print("\n2. Creating environment...")
    env = EVDepotEnv(
        dataset,
        peak_penalty_coef=100.0,
        late_penalty_coef=500.0,
        battery_cycle_cost=0.05,
        render_mode='human'
    )
    
    # Check environment
    print("\n3. Checking environment compatibility...")
    from gymnasium.utils.env_checker import check_env
    check_env(env, warn=True)
    print("   ✅ Environment is Gymnasium-compatible!")
    
    # Test random policy
    print("\n4. Testing with random policy...")
    obs, info = env.reset(seed=42)
    print(f"   Initial observation shape: {obs.shape}")
    print(f"   Initial info: {info}")
    
    total_reward = 0
    for step in range(100):  # Run 100 steps
        action = env.action_space.sample()
        obs, reward, terminated, truncated, info = env.step(action)
        total_reward += reward
        
        if terminated or truncated:
            break
    
    print(f"\n   Completed {step + 1} steps")
    print(f"   Total reward: {total_reward:.2f}")
    print(f"   Final cost: €{-total_reward:.2f}")
    
    # Get episode metrics
    print("\n5. Episode metrics:")
    metrics = env.get_episode_metrics()
    for key, value in metrics.items():
        print(f"   {key}: {value}")
    
    print("\n" + "="*70)
    print("✅ Environment test complete!")
    print("="*70)
    print("\nNext steps:")
    print("  - Use this environment with train_rl_agents.py")
    print("  - Train PPO/SAC/Lagrangian-PPO agents")
    print("  - Compare performance against baselines")
    print("="*70)